{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rito43/Rito43/blob/main/M23CSA021_Ass_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is useful in data loading, visualization and exploration. You are free to modify the code. The code has dependecy on Pytorch Lightning data module. However, you may use Pytorch as well."
      ],
      "metadata": {
        "id": "p4D2_W9OLZON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Unmount Google Drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "e8xu6RXzL3Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "GYJxoAQdLztb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDk8pPZjL2qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Dataset**\n",
        "\n",
        "The data has a total of 10 classes with 40 samples each. Make sure while working with the data, **esc10=True**. In the assignment, you are required to perform 4-fold validation. This dataset has been already divided into 5-folds. The column 'fold' in the metafile denotes the sample in a particular fold. Moreover, first folds is considered for test, rest for 4-fold validation."
      ],
      "metadata": {
        "id": "_HOr0uxaMG_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DL Assignment 2\n",
        "# Authors: Kopal Rastogi, Ishan Mishra\n",
        "# Keywords: None\n",
        "# Assumptions: None"
      ],
      "metadata": {
        "id": "y7DAfIt27G-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the requirements\n",
        "print('Installing Requirements... ',end='')\n",
        "!pip install lightning\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "B0yrDVM0R7Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "#with zipfile.ZipFile(\"/content/master.zip\", 'r') as zip_ref:\n",
        "    #zip_ref.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "bYS2KYTZSOFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "print('Importing Libraries... ',end='')\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import zipfile\n",
        "from torchaudio.transforms import Resample\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "zbjpBLpc8sno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "print('Downlading data... ', end='')\n",
        "# Your code here\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "kh-C8E00Nic0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/Archive.zip'\n",
        "extract_path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "tHT-CdI2ePIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "file_name = 'Archive.zip'\n",
        "file_path = os.path.join(drive_path, file_name)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists at:\", file_path)\n",
        "else:\n",
        "    print(\"File not found:\", file_path)"
      ],
      "metadata": {
        "id": "0-oKNacFef1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "FHoVq9fM7EHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "path = Path('/content/drive/MyDrive/')\n",
        "df = pd.read_csv('/content/drive/MyDrive/meta/esc50.csv')"
      ],
      "metadata": {
        "id": "VttdfD-p8gfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting list of raw audio files\n",
        "wavs = list(path.glob('audio/*'))  # List all audio files in the 'audio' directory using pathlib.Path.glob\n",
        "\n",
        "# Visualizing data\n",
        "waveform, sample_rate = torchaudio.load(wavs[0])  # Load the waveform and sample rate of the first audio file using torchaudio\n",
        "\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))  # Print the shape of the waveform tensor\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))  # Print the sample rate of the audio file\n",
        "\n",
        "# Plot the waveform using matplotlib\n",
        "plt.figure()\n",
        "plt.plot(waveform.t().numpy())  # Transpose and convert the waveform tensor to a NumPy array for plotting\n",
        "\n",
        "# Display the audio using IPython.display.Audio\n",
        "ipd.Audio(waveform, rate=sample_rate)  # Create an interactive audio player for the loaded waveform\n"
      ],
      "metadata": {
        "id": "V_SDQQJfNK-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        # Initialize CustomDataset object with relevant parameters\n",
        "        # dataset: \"train\", \"val\", or \"test\"\n",
        "        # kwargs: Additional parameters like data directory, dataframe, folds, etc.\n",
        "\n",
        "        # Extract parameters from kwargs\n",
        "        self.data_directory = kwargs[\"data_directory\"]\n",
        "        self.data_frame = kwargs[\"data_frame\"]\n",
        "        self.validation_fold = kwargs[\"validation_fold\"]\n",
        "        self.testing_fold = kwargs[\"testing_fold\"]\n",
        "        self.esc_10_flag = kwargs[\"esc_10_flag\"]\n",
        "        self.file_column = kwargs[\"file_column\"]\n",
        "        self.label_column = kwargs[\"label_column\"]\n",
        "        self.sampling_rate = kwargs[\"sampling_rate\"]\n",
        "        self.new_sampling_rate = kwargs[\"new_sampling_rate\"]\n",
        "        self.sample_length_seconds = kwargs[\"sample_length_seconds\"]\n",
        "\n",
        "        # Filter dataframe based on esc_10_flag and data_type\n",
        "        if self.esc_10_flag:\n",
        "            self.data_frame = self.data_frame.loc[self.data_frame['esc10'] == True]\n",
        "\n",
        "        if dataset == \"train\":\n",
        "            self.data_frame = self.data_frame.loc[\n",
        "                (self.data_frame['fold'] != self.validation_fold) & (self.data_frame['fold'] != self.testing_fold)]\n",
        "        elif dataset == \"val\":\n",
        "            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.validation_fold]\n",
        "        elif dataset == \"test\":\n",
        "            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.testing_fold]\n",
        "\n",
        "        # Get unique categories from the filtered dataframe\n",
        "        self.categories = sorted(self.data_frame[self.label_column].unique())\n",
        "\n",
        "        # Initialize lists to hold file names, labels, and folder numbers\n",
        "        self.file_names = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Initialize dictionaries for category-to-index and index-to-category mapping\n",
        "        self.category_to_index = {}\n",
        "        self.index_to_category = {}\n",
        "\n",
        "        for i, category in enumerate(self.categories):\n",
        "            self.category_to_index[category] = i\n",
        "            self.index_to_category[i] = category\n",
        "\n",
        "        # Populate file names and labels lists by iterating through the dataframe\n",
        "        for ind in tqdm(range(len(self.data_frame))):\n",
        "            row = self.data_frame.iloc[ind]\n",
        "            file_path = self.data_directory / \"audio\" / row[self.file_column]\n",
        "            self.file_names.append(file_path)\n",
        "            self.labels.append(self.category_to_index[row[self.label_column]])\n",
        "\n",
        "        self.resampler = torchaudio.transforms.Resample(self.sampling_rate, self.new_sampling_rate)\n",
        "\n",
        "        # Window size for rolling window sample splits (unfold method)\n",
        "        if self.sample_length_seconds == 2:\n",
        "            self.window_size = self.new_sampling_rate * 2\n",
        "            self.step_size = int(self.new_sampling_rate * 0.75)\n",
        "        else:\n",
        "            self.window_size = self.new_sampling_rate\n",
        "            self.step_size = int(self.new_sampling_rate * 0.5)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Split audio files with overlap, pass as stacked tensors tensor with a single label\n",
        "        path = self.file_names[index]\n",
        "        audio_file = torchaudio.load(path, format=None, normalize=True)\n",
        "        audio_tensor = self.resampler(audio_file[0])\n",
        "        splits = audio_tensor.unfold(1, self.window_size, self.step_size)\n",
        "        samples = splits.permute(1, 0, 2)\n",
        "        return samples, self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n"
      ],
      "metadata": {
        "id": "3X0MFpA_RNKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Initialize the CustomDataModule with batch size, number of workers, and other parameters\n",
        "        super().__init__()\n",
        "        self.batch_size = kwargs[\"batch_size\"]\n",
        "        self.num_workers = kwargs[\"num_workers\"]\n",
        "        self.data_module_kwargs = kwargs\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Define datasets for training, validation, and testing during Lightning setup\n",
        "\n",
        "        # If in 'fit' or None stage, create training and validation datasets\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.training_dataset = CustomDataset(dataset=\"train\", **self.data_module_kwargs)\n",
        "            self.validation_dataset = CustomDataset(dataset=\"val\", **self.data_module_kwargs)\n",
        "\n",
        "        # If in 'test' or None stage, create testing dataset\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.testing_dataset = CustomDataset(dataset=\"test\", **self.data_module_kwargs)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Return DataLoader for training dataset\n",
        "        return DataLoader(self.training_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # Return DataLoader for validation dataset\n",
        "        return DataLoader(self.validation_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # Return DataLoader for testing dataset\n",
        "        return DataLoader(self.testing_dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=self.collate_function,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def collate_function(self, data):\n",
        "        \"\"\"\n",
        "        Collate function to process a batch of examples and labels.\n",
        "\n",
        "        Args:\n",
        "            data: a tuple of 2 tuples with (example, label) where\n",
        "                example are the split 1 second sub-frame audio tensors per file\n",
        "                label = the label\n",
        "\n",
        "        Returns:\n",
        "            A list containing examples (concatenated tensors) and labels (flattened tensor).\n",
        "        \"\"\"\n",
        "        examples, labels = zip(*data)\n",
        "        examples = torch.stack(examples)\n",
        "        examples = examples.reshape(examples.size(0),1,-1)\n",
        "        labels = torch.flatten(torch.tensor(labels))\n",
        "\n",
        "        return [examples, labels]\n"
      ],
      "metadata": {
        "id": "r2JUJdPTRjix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Setup\n",
        "\n",
        "test_samp = 1  # Do not change this!!\n",
        "valid_samp = 2 # Use any value ranging from 2 to 5 for k-fold validation (valid_fold)\n",
        "batch_size = 32 # Free to change\n",
        "num_workers = 2 # Free to change\n",
        "custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                      num_workers=num_workers,\n",
        "                                      data_directory=path,\n",
        "                                      data_frame=df,\n",
        "                                      validation_fold=valid_samp,\n",
        "                                      testing_fold=test_samp,  # set to 0 for no test set\n",
        "                                      esc_10_flag=True,\n",
        "                                      file_column='filename',\n",
        "                                      label_column='category',\n",
        "                                      sampling_rate=44100,\n",
        "                                      new_sampling_rate=16000,  # new sample rate for input\n",
        "                                      sample_length_seconds=1  # new length of input in seconds\n",
        "                                      )\n",
        "\n",
        "custom_data_module.setup()\n"
      ],
      "metadata": {
        "id": "Irm0dFPaS1JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "print('Class Label: ', custom_data_module.training_dataset[0][1])  # this prints the class label\n",
        "print('Shape of data sample tensor: ', custom_data_module.training_dataset[0][0].shape)  # this prints the shape of the sample (Frames, Channel, Features)\n"
      ],
      "metadata": {
        "id": "ZW9GhLFjUayx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader(s)\n",
        "x = next(iter(custom_data_module.train_dataloader()))\n",
        "y = next(iter(custom_data_module.val_dataloader()))\n",
        "z = next(iter(custom_data_module.test_dataloader()))\n",
        "print('Train Dataloader:')\n",
        "print(x)\n",
        "print('Validation Dataloader:')\n",
        "print(y)\n",
        "print('Test Dataloader:')\n",
        "print(z)\n"
      ],
      "metadata": {
        "id": "61td-GUuUcpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "Yuf0MFlZldJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "\n"
      ],
      "metadata": {
        "id": "XRRbTAm7nBWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "xrA16GWtlQKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "import wandb\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=16, stride=1, padding=8)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1, padding=8)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=16, stride=1, padding=8)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool1d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool1d(self.conv2(x), 2))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 128)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNNModule(pl.LightningModule):\n",
        "    def __init__(self, model, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        val_loss = F.cross_entropy(y_hat, y)\n",
        "        self.log('val_loss', val_loss)\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "cnn_model = CNNModel(num_classes)\n",
        "cnn_module = CNNModule(cnn_model)\n",
        "\n",
        "\n",
        "wandb_logger = pl.loggers.WandbLogger(project='your_project_name', log_model=True)\n",
        "\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=3, logger=wandb_logger)\n",
        "\n",
        "\n",
        "trainer.fit(cnn_module, custom_data_module)"
      ],
      "metadata": {
        "id": "9dHKTitGXYf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F8nZIueuspSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "\n",
        "k = 4\n",
        "fold_results = []\n",
        "\n",
        "for fold in range(k):\n",
        "    print(f\"Fold {fold + 1}/{k}\")\n",
        "    custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                          num_workers=num_workers,\n",
        "                                          data_directory=path,\n",
        "                                          data_frame=df,\n",
        "                                          validation_fold=(fold + 1),\n",
        "                                          testing_fold=test_samp,\n",
        "                                          esc_10_flag=True,\n",
        "                                          file_column='filename',\n",
        "                                          label_column='category',\n",
        "                                          sampling_rate=44100,\n",
        "                                          new_sampling_rate=16000,\n",
        "                                          sample_length_seconds=1)\n",
        "\n",
        "    custom_data_module.setup()\n",
        "\n",
        "\n",
        "    trainer.fit(cnn_module, custom_data_module)\n",
        "\n",
        "\n",
        "    result = trainer.test(cnn_module, custom_data_module.test_dataloader())\n",
        "    fold_results.append(result)\n",
        "\n",
        "\n",
        "test_results = {}\n",
        "for i, result in enumerate(fold_results):\n",
        "    test_results[f'Fold_{i+1}'] = result[0]\n",
        "\n",
        "\n",
        "test_loss = np.mean([result['test_loss'] for result in fold_results])\n",
        "print(\"Overall Test Loss:\", test_loss)\n",
        "\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for fold in range(k):\n",
        "    custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                          num_workers=num_workers,\n",
        "                                          data_directory=path,\n",
        "                                          data_frame=df,\n",
        "                                          validation_fold=(fold + 1),\n",
        "                                          testing_fold=test_samp,\n",
        "                                          esc_10_flag=True,\n",
        "                                          file_column='filename',\n",
        "                                          label_column='category',\n",
        "                                          sampling_rate=44100,\n",
        "                                          new_sampling_rate=16000,\n",
        "                                          sample_length_seconds=1)\n",
        "    custom_data_module.setup()\n",
        "    preds = trainer.predict(cnn_module, custom_data_module.test_dataloader())\n",
        "    preds = np.concatenate([p.cpu().numpy() for p in preds])\n",
        "    true_labels = np.concatenate([labels.cpu().numpy() for _, labels in custom_data_module.test_dataloader()])\n",
        "    all_preds.append(preds)\n",
        "    all_labels.append(true_labels)\n",
        "\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=custom_data_module.categories, yticklabels=custom_data_module.categories)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=custom_data_module.categories)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "\n",
        "f1_scores = classification_report(all_labels, all_preds, target_names=custom_data_module.categories, output_dict=True)['f1-score']\n",
        "print(\"F1-Scores:\")\n",
        "print(f1_scores)\n",
        "\n",
        "\n",
        "auc_roc_score = roc_auc_score(pd.get_dummies(all_labels), pd.get_dummies(all_preds), average='macro')\n",
        "print(\"AUC-ROC Score:\", auc_roc_score)\n"
      ],
      "metadata": {
        "id": "a8ilsq0YuueX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in cnn_module.parameters())\n",
        "trainable_params = sum(p.numel() for p in cnn_module.parameters() if p.requires_grad)\n",
        "non_trainable_params = total_params - trainable_params\n",
        "print(\"Total Trainable Parameters:\", trainable_params)\n",
        "print(\"Total Non-Trainable Parameters:\", non_trainable_params)\n",
        "\n",
        "\n",
        "lr_values = [1e-2, 1e-3, 1e-4]\n",
        "best_lr = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for lr in lr_values:\n",
        "\n",
        "    wandb_logger = pl.loggers.WandbLogger(project='your_project_name', log_model=True, name=f'lr_{lr}')\n",
        "\n",
        "\n",
        "    trainer = pl.Trainer(max_epochs=3, logger=wandb_logger)\n",
        "\n",
        "\n",
        "    cnn_module.lr = lr\n",
        "\n",
        "\n",
        "    trainer.fit(cnn_module, custom_data_module)\n",
        "\n",
        "\n",
        "    result = trainer.test(cnn_module, custom_data_module.test_dataloader())\n",
        "\n",
        "\n",
        "    accuracy = result[0]['test_acc']\n",
        "\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_lr = lr\n",
        "\n",
        "print(\"Best Learning Rate:\", best_lr)\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "id": "88KHUADeCKhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, input_size, num_heads=1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(input_size, num_heads)\n",
        "        self.norm = nn.LayerNorm(input_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_size, 4 * input_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * input_size, input_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.self_attention(x, x, x)\n",
        "        x = self.norm(x + attn_output)\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.norm(x + mlp_output)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_size, num_heads=1, num_blocks=2):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(input_size, num_heads) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTransformer(pl.LightningModule):\n",
        "    def __init__(self, num_classes, input_channels, input_length, num_heads=[1, 2, 4]):\n",
        "        super(ConvTransformer, self).__init__()\n",
        "        self.cnn_base = CNNModel(num_classes)\n",
        "        self.transformer_heads = nn.ModuleList([\n",
        "            TransformerEncoder(input_size=256, num_heads=num_head) for num_head in num_heads\n",
        "        ])\n",
        "        self.mlp_head = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_base(x)\n",
        "        for transformer_head in self.transformer_heads:\n",
        "            x = transformer_head(x)\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTransformerModule(pl.LightningModule):\n",
        "    def __init__(self, model, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        val_loss = F.cross_entropy(y_hat, y)\n",
        "        self.log('val_loss', val_loss)\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "def evaluate_on_test_set(model, test_dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            x, y = batch\n",
        "            y_hat = model(x)\n",
        "            preds = torch.argmax(y_hat, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_preds, average='weighted', multi_class='ovr')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, f1, auc_roc, cm\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 8\n",
        "custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                      num_workers=num_workers,\n",
        "                                      data_directory=path,\n",
        "                                      data_frame=df,\n",
        "                                      validation_fold=1,\n",
        "                                      testing_fold=2,\n",
        "                                      esc_10_flag=True,\n",
        "                                      file_column='filename',\n",
        "                                      label_column='category',\n",
        "                                      sampling_rate=44100,\n",
        "                                      new_sampling_rate=16000,\n",
        "                                      sample_length_seconds=1\n",
        "                                      )\n",
        "\n",
        "\n",
        "kfolds = 4\n",
        "kf = KFold(n_splits=kfolds, shuffle=True)\n",
        "test_accuracies = []\n",
        "test_f1_scores = []\n",
        "test_auc_rocs = []\n",
        "confusion_matrices = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(custom_data_module)):\n",
        "    print(f\"Fold {fold+1}/{kfolds}\")\n",
        "\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(custom_data_module, train_idx)\n",
        "    val_dataset = torch.utils.data.Subset(custom_data_module, val_idx)\n",
        "\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    conv_transformer_model = ConvTransformer(num_classes, input_channels=1, input_length=16000)\n",
        "    conv_transformer_module = ConvTransformerModule(conv_transformer_model)\n",
        "    trainer = pl.Trainer(max_epochs=3, logger=wandb_logger)\n",
        "\n",
        "\n",
        "    trainer.fit(conv_transformer_module, train_dataloader, val_dataloader)\n",
        "\n",
        "\n",
        "    test_dataloader = custom_data_module.test_dataloader()\n",
        "    accuracy, f1, auc_roc, cm = evaluate_on_test_set(conv_transformer_model, test_dataloader)\n",
        "\n",
        "\n",
        "    test_accuracies.append(accuracy)\n",
        "    test_f1_scores.append(f1)\n",
        "    test_auc_rocs.append(auc_roc)\n",
        "    confusion_matrices.append(cm)\n",
        "\n",
        "\n",
        "mean_accuracy = np.mean(test_accuracies)\n",
        "std_accuracy = np.std(test_accuracies)\n",
        "mean_f1 = np.mean(test_f1_scores)\n",
        "std_f1 = np.std(test_f1_scores)\n",
        "mean_auc_roc = np.mean(test_auc_rocs)\n",
        "std_auc_roc = np.std(test_auc_rocs)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy} ± {std_accuracy}\")\n",
        "print(f\"Mean F1 Score: {mean_f1} ± {std_f1}\")\n",
        "print(f\"Mean AUC-ROC Score: {mean_auc_roc} ± {std_auc_roc}\")\n",
        "\n",
        "\n",
        "mean_cm = np.mean(confusion_matrices, axis=0)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(mean_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=custom_data_module.testing_dataset.categories, yticklabels=custom_data_module.testing_dataset.categories)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Mean Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "urTtOfqLMxq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv_transformer_model = ConvTransformer(num_classes, input_channels=1, input_length=16000)\n",
        "\n",
        "\n",
        "trainable_params = sum(p.numel() for p in conv_transformer_model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "non_trainable_params = sum(p.numel() for p in conv_transformer_model.parameters() if not p.requires_grad)\n",
        "\n",
        "print(f\"Total Trainable Parameters: {trainable_params}\")\n",
        "print(f\"Total Non-Trainable Parameters: {non_trainable_params}\")\n"
      ],
      "metadata": {
        "id": "dfHhK2V-Nx9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "learning_rates = [1e-3, 5e-4, 1e-4]\n",
        "batch_sizes = [16, 32, 64]\n",
        "num_heads_options = [[1], [2], [4], [1, 2], [2, 4], [1, 2, 4]]\n",
        "\n",
        "\n",
        "best_hyperparameters = None\n",
        "best_metrics = {\n",
        "    'accuracy': 0,\n",
        "    'f1_score': 0,\n",
        "    'auc_roc': 0\n",
        "}\n",
        "\n",
        "\n",
        "for lr, batch_size, num_heads in itertools.product(learning_rates, batch_sizes, num_heads_options):\n",
        "    print(f\"Testing hyperparameters: LR={lr}, Batch Size={batch_size}, Num Heads={num_heads}\")\n",
        "\n",
        "\n",
        "    custom_data_module = CustomDataModule(batch_size=batch_size,\n",
        "                                          num_workers=num_workers,\n",
        "                                          data_directory=path,\n",
        "                                          data_frame=df,\n",
        "                                          validation_fold=1,\n",
        "                                          testing_fold=2,\n",
        "                                          esc_10_flag=True,\n",
        "                                          file_column='filename',\n",
        "                                          label_column='category',\n",
        "                                          sampling_rate=44100,\n",
        "                                          new_sampling_rate=16000,\n",
        "                                          sample_length_seconds=1\n",
        "                                          )\n",
        "\n",
        "\n",
        "    kfolds = 4\n",
        "    kf = KFold(n_splits=kfolds, shuffle=True)\n",
        "    test_accuracies = []\n",
        "    test_f1_scores = []\n",
        "    test_auc_rocs = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(custom_data_module)):\n",
        "\n",
        "\n",
        "        train_dataset = torch.utils.data.Subset(custom_data_module, train_idx)\n",
        "        val_dataset = torch.utils.data.Subset(custom_data_module, val_idx)\n",
        "\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "        conv_transformer_model = ConvTransformer(num_classes, input_channels=1, input_length=16000, num_heads=num_heads)\n",
        "        conv_transformer_module = ConvTransformerModule(conv_transformer_model, lr=lr)\n",
        "        trainer = pl.Trainer(max_epochs=3, logger=wandb_logger)\n",
        "\n",
        "\n",
        "        trainer.fit(conv_transformer_module, train_dataloader, val_dataloader)\n",
        "\n",
        "\n",
        "        test_dataloader = custom_data_module.test_dataloader()\n",
        "        accuracy, f1, auc_roc, _ = evaluate_on_test_set(conv_transformer_model, test_dataloader)\n",
        "\n",
        "\n",
        "        test_accuracies.append(accuracy)\n",
        "        test_f1_scores.append(f1)\n",
        "        test_auc_rocs.append(auc_roc)\n",
        "\n",
        "\n",
        "    mean_accuracy = np.mean(test_accuracies)\n",
        "    mean_f1 = np.mean(test_f1_scores)\n",
        "    mean_auc_roc = np.mean(test_auc_rocs)\n",
        "\n",
        "\n",
        "    print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "    print(f\"Mean F1 Score: {mean_f1}\")\n",
        "    print(f\"Mean AUC-ROC Score: {mean_auc_roc}\")\n",
        "\n",
        "\n",
        "    if mean_accuracy > best_metrics['accuracy']:\n",
        "        best_metrics['accuracy'] = mean_accuracy\n",
        "        best_metrics['f1_score'] = mean_f1\n",
        "        best_metrics['auc_roc'] = mean_auc_roc\n",
        "        best_hyperparameters = {\n",
        "            'learning_rate': lr,\n",
        "            'batch_size': batch_size,\n",
        "            'num_heads': num_heads\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_hyperparameters)\n",
        "print(\"Corresponding Metrics:\")\n",
        "print(best_metrics)\n"
      ],
      "metadata": {
        "id": "wlDrg_YdN20W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}